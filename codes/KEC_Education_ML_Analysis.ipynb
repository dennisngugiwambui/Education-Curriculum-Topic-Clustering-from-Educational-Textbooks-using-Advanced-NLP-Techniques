{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KEC Educational Content Analysis with Machine Learning\n",
    "## Web Scraping and Classification System for Form 1-4 Content\n",
    "\n",
    "This notebook scrapes educational content from https://kec.ac.ke/ and builds ML models for content classification and question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (2.32.3)\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.13.5-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting selenium\n",
      "  Downloading selenium-4.35.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (2.2.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (1.6.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (3.9.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (4.50.3)\n",
      "Requirement already satisfied: torch in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (2.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (from requests) (2024.12.14)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4)\n",
      "  Downloading soupsieve-2.8-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (from beautifulsoup4) (4.12.2)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting trio~=0.30.0 (from selenium)\n",
      "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting trio-websocket~=0.12.2 (from selenium)\n",
      "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Downloading certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting typing-extensions>=4.0.0 (from beautifulsoup4)\n",
      "  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting websocket-client~=1.8.0 (from selenium)\n",
      "  Downloading websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: click in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.30.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (from torch) (78.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->nltk) (0.4.6)\n",
      "Collecting attrs>=23.2.0 (from trio~=0.30.0->selenium)\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting sortedcontainers (from trio~=0.30.0->selenium)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting outcome (from trio~=0.30.0->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
      "Collecting cffi>=1.14 (from trio~=0.30.0->selenium)\n",
      "  Downloading cffi-1.17.1-cp312-cp312-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.12.2->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pysocks!=1.5.7,<2.0,>=1.5.6 (from urllib3[socks]<3.0,>=2.5.0->selenium)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Collecting pycparser (from cffi>=1.14->trio~=0.30.0->selenium)\n",
      "  Downloading pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.14.0)\n",
      "Downloading beautifulsoup4-4.13.5-py3-none-any.whl (105 kB)\n",
      "Downloading selenium-4.35.0-py3-none-any.whl (9.6 MB)\n",
      "   ---------------------------------------- 0.0/9.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/9.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/9.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/9.6 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/9.6 MB 929.6 kB/s eta 0:00:10\n",
      "   --- ------------------------------------ 0.8/9.6 MB 958.5 kB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 1.0/9.6 MB 986.7 kB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 1.3/9.6 MB 1.0 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 1.6/9.6 MB 1.0 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 1.6/9.6 MB 1.0 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 1.8/9.6 MB 1.0 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 2.1/9.6 MB 1.1 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 2.6/9.6 MB 1.1 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 2.6/9.6 MB 1.1 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 2.9/9.6 MB 1.1 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 2.9/9.6 MB 1.1 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 2.9/9.6 MB 1.1 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 3.4/9.6 MB 1.0 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 3.7/9.6 MB 1.0 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 3.9/9.6 MB 1.0 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 4.2/9.6 MB 1.0 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 4.5/9.6 MB 1.0 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 4.7/9.6 MB 1.1 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 5.0/9.6 MB 1.1 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 5.2/9.6 MB 1.1 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 5.2/9.6 MB 1.1 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 5.5/9.6 MB 1.1 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 5.8/9.6 MB 1.1 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 6.0/9.6 MB 1.1 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 6.6/9.6 MB 1.1 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 6.6/9.6 MB 1.1 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 6.8/9.6 MB 1.1 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 7.1/9.6 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 7.3/9.6 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 7.3/9.6 MB 1.1 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 7.9/9.6 MB 1.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 8.1/9.6 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 8.4/9.6 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 8.7/9.6 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.9/9.6 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.4/9.6 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.6/9.6 MB 1.1 MB/s eta 0:00:00\n",
      "Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Downloading soupsieve-2.8-py3-none-any.whl (36 kB)\n",
      "Downloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
      "Downloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
      "Downloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading cffi-1.17.1-cp312-cp312-win_amd64.whl (181 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Downloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: sortedcontainers, wsproto, websocket-client, urllib3, typing-extensions, soupsieve, pysocks, pycparser, certifi, attrs, outcome, cffi, beautifulsoup4, trio, trio-websocket, selenium\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.3.0\n",
      "    Uninstalling urllib3-2.3.0:\n",
      "      Successfully uninstalled urllib3-2.3.0\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.12.2\n",
      "    Uninstalling typing_extensions-4.12.2:\n",
      "      Successfully uninstalled typing_extensions-4.12.2\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2024.12.14\n",
      "    Uninstalling certifi-2024.12.14:\n",
      "      Successfully uninstalled certifi-2024.12.14\n",
      "Successfully installed attrs-25.3.0 beautifulsoup4-4.13.5 certifi-2025.8.3 cffi-1.17.1 outcome-1.3.0.post0 pycparser-2.22 pysocks-1.7.1 selenium-4.35.0 sortedcontainers-2.4.0 soupsieve-2.8 trio-0.30.0 trio-websocket-0.12.2 typing-extensions-4.14.1 urllib3-2.5.0 websocket-client-1.8.0 wsproto-1.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting webdriver-manager\n",
      "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting lxml\n",
      "  Downloading lxml-6.0.1-cp312-cp312-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.6-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.9.4-cp312-cp312-win_amd64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (from webdriver-manager) (2.32.3)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (from webdriver-manager) (1.0.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (from webdriver-manager) (24.2)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.3-cp312-cp312-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.59.2-cp312-cp312-win_amd64.whl.metadata (111 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.9-cp312-cp312-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (2.2.4)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-11.3.0-cp312-cp312-win_amd64.whl.metadata (9.2 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (from requests->webdriver-manager) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (from requests->webdriver-manager) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (from requests->webdriver-manager) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\denno\\appdata\\roaming\\python\\python312\\site-packages (from requests->webdriver-manager) (2025.8.3)\n",
      "Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
      "Downloading lxml-6.0.1-cp312-cp312-win_amd64.whl (4.0 MB)\n",
      "   ---------------------------------------- 0.0/4.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/4.0 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.3/4.0 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.8/4.0 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 1.3/4.0 MB 1.9 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 2.1/4.0 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 2.9/4.0 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 3.7/4.0 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.0/4.0 MB 2.9 MB/s eta 0:00:00\n",
      "Downloading matplotlib-3.10.6-cp312-cp312-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/8.1 MB 5.6 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.3/8.1 MB 3.7 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.1/8.1 MB 3.5 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.1/8.1 MB 3.5 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.6/8.1 MB 2.4 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.1/8.1 MB 2.5 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 3.9/8.1 MB 2.8 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 4.5/8.1 MB 2.8 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 4.5/8.1 MB 2.8 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 4.7/8.1 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 5.0/8.1 MB 2.1 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 5.0/8.1 MB 2.1 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 5.0/8.1 MB 2.1 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 5.2/8.1 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 5.2/8.1 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 5.5/8.1 MB 1.7 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 5.5/8.1 MB 1.7 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 5.8/8.1 MB 1.5 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 5.8/8.1 MB 1.5 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 6.0/8.1 MB 1.4 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 6.3/8.1 MB 1.4 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 6.8/8.1 MB 1.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.9/8.1 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.1/8.1 MB 1.6 MB/s eta 0:00:00\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading wordcloud-1.9.4-cp312-cp312-win_amd64.whl (301 kB)\n",
      "Downloading contourpy-1.3.3-cp312-cp312-win_amd64.whl (226 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.59.2-cp312-cp312-win_amd64.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.5/2.3 MB 4.2 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.3/2.3 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.1/2.3 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.3/2.3 MB 3.3 MB/s eta 0:00:00\n",
      "Downloading kiwisolver-1.4.9-cp312-cp312-win_amd64.whl (73 kB)\n",
      "Downloading pillow-11.3.0-cp312-cp312-win_amd64.whl (7.0 MB)\n",
      "   ---------------------------------------- 0.0/7.0 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.8/7.0 MB 4.2 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 1.6/7.0 MB 4.2 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 2.4/7.0 MB 4.5 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 2.4/7.0 MB 4.5 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 2.4/7.0 MB 4.5 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 3.1/7.0 MB 2.6 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 3.9/7.0 MB 2.6 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 4.5/7.0 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 5.0/7.0 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.5/7.0 MB 2.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.8/7.0 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.6/7.0 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.6/7.0 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.0/7.0 MB 2.3 MB/s eta 0:00:00\n",
      "Downloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: pyparsing, pillow, lxml, kiwisolver, fonttools, cycler, contourpy, webdriver-manager, matplotlib, wordcloud, seaborn\n",
      "Successfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.59.2 kiwisolver-1.4.9 lxml-6.0.1 matplotlib-3.10.6 pillow-11.3.0 pyparsing-3.2.3 seaborn-0.13.2 webdriver-manager-4.0.2 wordcloud-1.9.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install requests beautifulsoup4 selenium pandas numpy scikit-learn nltk transformers torch\n",
    "!pip install webdriver-manager lxml matplotlib seaborn wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML and NLP imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Denno/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Denno/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Denno/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Denno/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Web Scraping KEC Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KECWebScraper:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://kec.ac.ke/\"\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        })\n",
    "        self.scraped_data = []\n",
    "        \n",
    "    def setup_selenium_driver(self):\n",
    "        \"\"\"Setup Selenium WebDriver for dynamic content\"\"\"\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('--headless')\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        return webdriver.Chrome(service=service, options=options)\n",
    "    \n",
    "    def scrape_main_page(self):\n",
    "        \"\"\"Scrape the main KEC page to find educational content links\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(self.base_url)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Find education level sections\n",
    "            education_sections = {\n",
    "                'pre_primary': [],\n",
    "                'primary': [],\n",
    "                'junior_secondary': [],\n",
    "                'secondary': []\n",
    "            }\n",
    "            \n",
    "            # Look for grade/form links\n",
    "            links = soup.find_all('a', href=True)\n",
    "            for link in links:\n",
    "                href = link.get('href')\n",
    "                text = link.get_text().lower().strip()\n",
    "                \n",
    "                if any(grade in text for grade in ['form 1', 'form 2', 'form 3', 'form 4']):\n",
    "                    education_sections['secondary'].append({\n",
    "                        'url': urljoin(self.base_url, href),\n",
    "                        'title': link.get_text().strip(),\n",
    "                        'level': 'secondary'\n",
    "                    })\n",
    "            \n",
    "            return education_sections\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping main page: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def scrape_content_page(self, url, title, level):\n",
    "        \"\"\"Scrape individual content pages\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(url)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Extract text content\n",
    "            content_text = \"\"\n",
    "            \n",
    "            # Remove script and style elements\n",
    "            for script in soup([\"script\", \"style\"]):\n",
    "                script.decompose()\n",
    "            \n",
    "            # Get text content\n",
    "            text = soup.get_text()\n",
    "            lines = (line.strip() for line in text.splitlines())\n",
    "            chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "            content_text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "            \n",
    "            return {\n",
    "                'url': url,\n",
    "                'title': title,\n",
    "                'level': level,\n",
    "                'content': content_text,\n",
    "                'word_count': len(content_text.split()),\n",
    "                'scraped_at': time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {url}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def scrape_all_content(self):\n",
    "        \"\"\"Main method to scrape all educational content\"\"\"\n",
    "        print(\"Starting KEC website scraping...\")\n",
    "        \n",
    "        # Get main page structure\n",
    "        sections = self.scrape_main_page()\n",
    "        \n",
    "        # Scrape each section\n",
    "        for section_name, links in sections.items():\n",
    "            print(f\"Scraping {section_name} section...\")\n",
    "            for link_info in links:\n",
    "                content = self.scrape_content_page(\n",
    "                    link_info['url'], \n",
    "                    link_info['title'], \n",
    "                    link_info['level']\n",
    "                )\n",
    "                if content:\n",
    "                    self.scraped_data.append(content)\n",
    "                time.sleep(1)  # Be respectful to the server\n",
    "        \n",
    "        return self.scraped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting KEC website scraping...\n",
      "Scraping pre_primary section...\n",
      "Scraping primary section...\n",
      "Scraping junior_secondary section...\n",
      "Scraping secondary section...\n",
      "Scraped 4 pages of content\n",
      "                                                 url   title      level  \\\n",
      "0  https://lms.kec.ac.ke/course/index.php?categor...  Form 1  secondary   \n",
      "1  https://lms.kec.ac.ke/course/index.php?categor...  Form 2  secondary   \n",
      "2  https://lms.kec.ac.ke/course/index.php?categor...  Form 3  secondary   \n",
      "3  https://lms.kec.ac.ke/course/index.php?categor...  Form 4  secondary   \n",
      "\n",
      "                                             content  word_count  \\\n",
      "0  Digital Content | Kenya Education Cloud | Unli...        5375   \n",
      "1  Digital Content | Kenya Education Cloud | Unli...        5375   \n",
      "2  Digital Content | Kenya Education Cloud | Unli...        5375   \n",
      "3  Digital Content | Kenya Education Cloud | Unli...        5375   \n",
      "\n",
      "            scraped_at  \n",
      "0  2025-09-07 23:50:27  \n",
      "1  2025-09-07 23:50:28  \n",
      "2  2025-09-07 23:50:30  \n",
      "3  2025-09-07 23:50:31  \n"
     ]
    }
   ],
   "source": [
    "# Initialize and run scraper\n",
    "scraper = KECWebScraper()\n",
    "scraped_content = scraper.scrape_all_content()\n",
    "\n",
    "print(f\"Scraped {len(scraped_content)} pages of content\")\n",
    "if scraped_content:\n",
    "    df_content = pd.DataFrame(scraped_content)\n",
    "    print(df_content.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
